# -*- coding: utf-8 -*-
"""Untitled30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12VtbWHXqfL51cKGBOxdH90Yulsfcf8ND
"""

!pip install OpenAI
!pip install LangChain
!pip install streamlit
!pip install langchain==0.0.148
!pip install pydantic==1.10.6
!pip install typing-inspect==0.8.0

!pip install git+https://github.com/hwchase17/langchain.git openai

!pip cache purge

!pip install git+https://github.com/hwchase17/langchain.git openai

!pip install langchain==0.0.148

!pip install pydantic==1.10.6
!pip install typing-inspect==0.8.0
!pip install langchain

!pip install openai

from langchain import PromptTemplate, LLMChain
from langchain.llms import OpenAI
import os

os.environ["OPENAI_API_KEY"] = "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"

prompt_template = "You are a helpful assistant. Answer the question: {question}"
prompt = PromptTemplate(template=prompt_template, input_variables=["question"])

llm = OpenAI(model="text-davinci-003")  # You can use other models like gpt-3.5-turbo
llm_chain = LLMChain(llm=llm, prompt=prompt)

question = "What is the capital of France?"
response = llm_chain.run({"question": question})
print(response)

!pip install streamlit

import streamlit as st
from langchain import OpenAI, LLMChain
from langchain.prompts import PromptTemplate
import os

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = "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"

# Define your LangChain app
prompt_template = "You are a helpful assistant. Answer the question: {question}"
prompt = PromptTemplate(template=prompt_template, input_variables=["question"])

# llm = OpenAI(model="text-davinci-003")
llm = OpenAI(model="gpt-3.5-turbo")
# llm = OpenAI(model_kwargs={"model": "gpt-3.5-turbo"})
llm_chain = LLMChain(llm=llm, prompt=prompt)

st.title("LangChain App")
question = st.text_input("Enter your question:")
if question:
    response = llm_chain.run({"question": question})
    st.write(f"Response: {response}")

!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py

# Commented out IPython magic to ensure Python compatibility.
# %load streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py

!pip install openai

!pip install langchain_community

!pip install LangChain

!pip install streamlit